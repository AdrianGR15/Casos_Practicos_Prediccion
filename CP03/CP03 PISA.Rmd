---
title: 'CP03 PISA'
author: "Adrián González Retamosa"
date: "9/11/2020"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.height = 5, fig.width = 10, fig.align = 'center')
```

# INDICE

1. Diccionario del Data Set
2. Objetivos del trabajo
3. Analisis y limpieza del Data Set
4. Eleccion grados polinomicos y grados de libertad con Cross Validation
5. Creacion de modelo GAM
6. Prediccion 
7. Conclusiones

## 1. Diccionario del Data Set

Overall - Nuestra variable dependiente, la nota media de ciencias 

Interest - El interés del alumno por la ciencia

Support - El apoyo a la investigación científica en el país

Income - Ea paridad de poder adquisitvio en dólares de 2005

health - Mide la inversion en sanidad del pais 

edu - Mide la inversion en educacion del pais

hdi - El indice de gini del pais 


## 2. Objetivos del trabajo

A traves de las diferentes variables construir un modelo GAM que nos permita predecir la nota media de cada detrminado pais en las asignaturas de ciencia. 


## 3. Analisis y limpieza del Data Set

Cargamos las librerias necesarias y el data set de trabajo 
```{r librerias}
library(readr)
library(prettydoc)
library(mgcv) 
library(glmnet)
library(boot)
library(rsample)
library(car)
library(skimr)
library(janitor) 
library(magrittr)
library(imputeTS)
library(tidyverse)
```

```{r carga del data set}
setwd('/Users/adrian_gr/Desktop/Data_Science/Prediccion/CP03/data')
df <- read_csv('PISA.csv')
```


## 3. Analisis y limpieza del Data Set

Eliminamos las variables que no vamos a utilizar en nuestro trabajo, armonizamos el estilo de escritiura de las variables y hacemos un resumen estadistico de las variables.
```{r analisis y limpieza del data set}
df <- df %>% 
  select(-c('Issues', 'Explain', 'Evidence')) #eliminamos las variables que no utilizaremos en el analisis 

df %<>% clean_names() #ponemos el nombre de las variables de forma estandar

colnames(df) #observamos los nombres de las variables 

skim(df) #observamos principales estadisticos y valores NaN

```

Sustituyo los NaN por la media de cada variable
```{r tratamiento valores NaN}
df %<>% na_mean() #con esta funcion convertimos los valores NaN en la media de ese regresor
```


Comprobamos que no hay datos atipicos por lo que no hay problema en imputar los valores NaN por las respectivas medias de cada variable.
```{r boxplot, fig.height = 7}
df_n <- df %>% 
  select(!'country') #eliminamos la variable categorica 

par(mfrow=c(3,3))

for (i in 1:7) {
  boxplot(df_n[,i], main=colnames(df_n)[i], type = "l", col="orange")
  abline(h=0, lty=2, lwd=1.5)
}

par(mfrow=c(1,1))
```


Realizamos un grafico de dispersion para comprobar como es la relacion entre los regresores y la variable dependiente.
```{r relacion entre regresores y variable dependiente}
df %>% tidyr::gather(key = 'regresor', value = 'id', 3:8) %>% 
  ggplot(aes(x = id, y = overall)) +
  geom_point(color = 'skyblue', alpha = 0.85) +
  
  geom_smooth(method = 'gam',
              se = FALSE,
              color = 'yellow') +
  
  facet_wrap(~regresor, ncol = 2, scales = 'free') 
```

Analizamos la multicolinealidad entre los regresores.
```{r estudio del VIF }
#detectar multicolinealidad a partir del factor de inflacion de la varianza en las variables numericas
model  <- lm(overall ~ ., data = df_n)
vif_valores <- car::vif(model)
barplot(vif_valores,
        main = "VIF por predictores",
        horiz = TRUE, 
        col = "skyblue")
abline(v = 5, lwd = 3, lty = 2, col = "red") # linea indicadora de problemas de VIF
kableExtra::kable(vif_valores)
```

Observamos que hay algunas variables que superan el valor 5 por lo que vamos a dejar fuera del modelo a las variables income, educ y health ya que estan incluidas en hdi. Comprobamos que con este modelo no habria problemas de multicolinealidad.
```{r hdi esta compuesto de health edu e income }
df_hdi <- df %>% 
  select('overall' ,'interest', 'support', 'hdi')
model2  <- lm(overall ~ ., data = df_hdi)
vif_valores2 <- car::vif(model2)
barplot(vif_valores2,
        main = "VIF por predictores",
        horiz = TRUE, 
        col = "skyblue")
abline(v = 5, lwd = 3, lty = 2, col = "red") # linea indicadora de problemas de VIF
kableExtra::kable(vif_valores2)
```


## 4. Eleccion grados polinomicos y grados de libertad con Cross Validation

A traves del cross validation calculamos cual seria el grado optimo para la variable income que como se observo en el grafico de dispersion puede no ser lienal.
```{r grado polinomio income}
#hacemos cross validation para averiguar el grado del polinomio en el regresor income que menos error genera 
set.seed(891) #aleatoriedad
error_cv <- data.frame(grado=seq(1,7,1),
                        error= rep(NA, 7)) #creamos un df para ir gurdardo los errores medio de cada polinomio 

for (i in 1:7) { 
  glm.fit <- glm(overall ~ poly(income, i), data=df) 
  error_cv$error[i] <- cv.glm(df, glm.fit, K=11)$delta[1]
} 
knitr::kable(error_cv) #observamos que el polinomio que menor error genera es el de grado 2 
```
En este grafico observamos como el error minimo se encontraria en el polinomio de segundo grado
```{r}
ggplot(error_cv, aes(x = grado, y = error)) +
  geom_line( col = 'red' ) 
```


Utilizamos tambien el cross validation para encontrar el grado de libertad optimo para cada variable y los almacenamos en una tabla.
```{r obtener de forma manuel grados libertad}
attach(df)
gl_interest <- smooth.spline(interest, overall)
gl_support <- smooth.spline(support, overall)
gl_income <- smooth.spline(income, overall)
gl_health <- smooth.spline(health, overall)
gl_edu <- smooth.spline(edu, overall)
gl_hdi <- smooth.spline(hdi, overall)
df_opt <- rbind(gl_interest$df, gl_support$df, gl_income$df, gl_health$df, gl_edu$df, gl_hdi$df )

regresor = colnames(df[3:8])
df_optimo <- as.data.frame(cbind(regresor, df_opt))

knitr::kable(df_optimo)
```


## 5. Creacion de modelo GAM

Creamos una muestra de entrenamiento y de test para poder comparar las predicciones de nuestros modelos 
```{r crecion train y test}
set.seed(891) 
df_split <- initial_split(df_n, 
                           prop = 0.85, 
                           strata = overall) 
df_train <- training(df_split) 
df_test <- testing(df_split) 
```


Generamos el primer modelo GAM con todos los regresores con splines.
```{r GAM con 6 splines}
#estimo el primer modelo gam con splines en todos los regresores
gam_1 <- gam(overall ~ s(interest) + s(support) + s(income) + s(health) + s(edu) + s(hdi), data = df_train)
summary(gam_1)

par(mfrow = c(2,3))
plot(gam_1, se=T, col='skyblue') #interest suppor y health 
```

En el grafico anterior observamos que hay variables que puede que sea mejor meter en el modelo sin splines y es lo que hacemos en este segundo modelo. Este modelo explica el 85,7% de la varinza de la variable dependiente.
```{r GAM con splines en income, edu y hdi}
gam_2 <- gam(overall ~ interest + support + s(income) + health + s(edu) + s(hdi), data = df_train)
summary(gam_2)

par(mfrow = c(1,3))
plot(gam_2, se=T, col='skyblue')
```

Por ultimo, vamos a correr otro modelo en el cual solo aparezaca la variable hdi, ya que engloba a las variables educ, health e income.
```{r GAM con hdi}
gam_hdi <- gam(overall ~ s(interest) + s(support)  + hdi, data = df_train)

par(mfrow = c(1,2))
plot(gam_hdi, se=T, col='skyblue')
summary(gam_hdi)
```


## 6. Prediccion

Construimos una table donde observamos los valores reales y las predicciones para la muestra de test de los 3 modelos construidos.
```{r}
pre_gam1 <- predict(gam_1, df_test)
pre_gam2 <- predict(gam_2, df_test)
pre_hdi <- predict(gam_hdi, df_test)

df_pre <- df_test %>% 
  select(overall)  %>% 
  mutate(pre_gam1 = pre_gam1,
         pre_gam2 = pre_gam2,
         pre_hdi = pre_hdi)

knitr::kable(df_pre)

```

Creamos otra tabla deonde podemos observar que es el segundo modelo, overall + interest + support + s(income) + health + s(edu) + s(hdi), es el que claramente tiene menor error de prediccion.
```{r estadisticos de errores}
attach(df_pre)

#GAM_1
gam_1_e <- rbind(mean(abs(pre_gam1 - overall )), sd(abs(pre_gam1 - overall )) )

#GAM_2
gam_2_e <- rbind(mean(abs(pre_gam2 - overall )), sd(abs(pre_gam2 - overall )))

#GAM_hdi
gam_hdi_e <- rbind(mean(abs(pre_hdi - overall )), sd(abs(pre_hdi - overall )))

errores_pred <- data.frame(gam1 = gam_1_e,
                           gam2 = gam_2_e,
                           gam_hdi = gam_hdi_e)

knitr::kable(errores_pred)
```


## 7. Conclusion
Despues de anlizar la relacion de cada uno de los regresores con la variable dependiente y correr 3 modelos GAM diferentes observamos que el modelo overall = interest + support + s(income) + health + s(edu) + s(hdi) es el que menor error de prediccion tiene.